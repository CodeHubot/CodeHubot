# 大模型API集成

## 概述

CodeHubot 项目集成了多种国内外主流大语言模型，提供统一的调用接口。本文档详细介绍大模型的集成方法、使用示例和最佳实践，适合教学和实际开发使用。

## 支持的大模型

### 国际模型

| 模型 | 提供商 | 特点 | Function Calling |
|------|--------|------|-----------------|
| **GPT-4** | OpenAI | 性能最强 | ✅ |
| **GPT-3.5-turbo** | OpenAI | 性价比高 | ✅ |
| **Claude** | Anthropic | 长文本支持 | ✅ |
| **Gemini** | Google | 多模态 | ✅ |

### 国产模型

| 模型 | 提供商 | 特点 | Function Calling |
|------|--------|------|-----------------|
| **通义千问** | 阿里云 | 中文理解好 | ✅ |
| **文心一言** | 百度 | 行业知识丰富 | ✅ |
| **讯飞星火** | 讯飞 | 语音能力强 | ✅ |
| **智谱GLM** | 智谱AI | 长文本处理 | ✅ |
| **月之暗面 Moonshot** | 月之暗面 | 超长上下文 | ✅ |
| **DeepSeek** | 深度求索 | 开源友好 | ✅ |
| **豆包** | 字节跳动 | 多轮对话好 | ✅ |

## 架构设计

### 统一调用架构

```
┌─────────────────────────────────────────────────────┐
│                应用层 (Agent/Workflow)               │
├─────────────────────────────────────────────────────┤
│                                                     │
│  ┌─────────────────────────────────────────────┐   │
│  │         LLM Service (统一接口)              │   │
│  │                                             │   │
│  │  def chat(messages, functions):            │   │
│  │      if provider == 'openai':              │   │
│  │          return call_openai_api()          │   │
│  │      elif provider == 'qwen':              │   │
│  │          return call_qwen_api()            │   │
│  │      elif provider == 'wenxin':            │   │
│  │          return call_wenxin_api()          │   │
│  │      ...                                   │   │
│  └─────────────────────────────────────────────┘   │
│                      │                              │
├──────────────────────┼──────────────────────────────┤
│                      ↓                              │
│  ┌──────┐  ┌──────┐  ┌──────┐  ┌──────┐           │
│  │OpenAI│  │ Qwen │  │Wenxin│  │ ...  │           │
│  │ API  │  │ API  │  │ API  │  │ API  │           │
│  └──────┘  └──────┘  └──────┘  └──────┘           │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 数据模型设计

```sql
-- LLM 提供商表
CREATE TABLE `llm_providers` (
  `id` INT PRIMARY KEY AUTO_INCREMENT,
  `name` VARCHAR(50) NOT NULL,        -- openai, qwen, wenxin
  `display_name` VARCHAR(100),        -- OpenAI, 通义千问, 文心一言
  `api_base` VARCHAR(255),            -- API 基础URL
  `is_active` TINYINT(1) DEFAULT 1,
  `created_at` DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- LLM 模型表
CREATE TABLE `llm_models` (
  `id` INT PRIMARY KEY AUTO_INCREMENT,
  `provider_id` INT,
  `name` VARCHAR(100) NOT NULL,       -- gpt-4, qwen-max
  `display_name` VARCHAR(100),        -- GPT-4, 通义千问Max
  `api_key` VARCHAR(255),             -- API密钥（加密存储）
  `api_base` VARCHAR(255),            -- 自定义API地址
  `temperature` DECIMAL(2,1) DEFAULT 0.7,
  `max_tokens` INT DEFAULT 4096,
  `top_p` DECIMAL(2,1) DEFAULT 0.9,
  `is_active` TINYINT(1) DEFAULT 1,
  `created_at` DATETIME DEFAULT CURRENT_TIMESTAMP,
  FOREIGN KEY (`provider_id`) REFERENCES `llm_providers`(`id`)
);
```

## OpenAI 集成

### 1. 基础配置

```python
# .env
OPENAI_API_KEY=sk-your-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1

# 或使用国内代理
# OPENAI_BASE_URL=https://api.openai-proxy.com/v1
```

### 2. 基本调用

```python
# services/llm_service.py
import requests
from typing import List, Dict, Any, Optional

class LLMService:
    def __init__(self, model):
        self.model = model
        self.api_base = model.api_base
        self.api_key = model.api_key
        self.model_name = model.name
        self.temperature = model.temperature or 0.7
        self.max_tokens = model.max_tokens or 4096
    
    def _call_openai_api(
        self,
        messages: List[Dict[str, str]],
        functions: Optional[List[Dict]] = None,
        function_call: Optional[str] = None
    ) -> Dict[str, Any]:
        """调用 OpenAI API"""
        url = f"{self.api_base}/chat/completions"
        
        payload = {
            "model": self.model_name,
            "messages": messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "top_p": 0.9
        }
        
        # Function Calling 支持
        if functions:
            payload["functions"] = functions
            payload["function_call"] = function_call or "auto"
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        response = requests.post(url, json=payload, headers=headers, timeout=60)
        response.raise_for_status()
        
        result = response.json()
        choice = result["choices"][0]
        message = choice["message"]
        
        output = {}
        if message.get("content"):
            output["response"] = message["content"]
        if message.get("function_call"):
            output["function_call"] = {
                "name": message["function_call"]["name"],
                "arguments": json.loads(message["function_call"]["arguments"])
            }
        
        return output
```

### 3. 使用示例

```python
# 简单对话
messages = [
    {"role": "system", "content": "你是一个友好的AI助手"},
    {"role": "user", "content": "介绍一下Python"}
]

result = llm_service.chat(messages)
print(result["response"])
```

## 通义千问集成

### 1. 配置

```python
# .env
DASHSCOPE_API_KEY=sk-your-dashscope-key
```

### 2. 调用实现

```python
def _call_qwen_api(
    self,
    messages: List[Dict[str, str]],
    functions: Optional[List[Dict]] = None,
    function_call: Optional[str] = None
) -> Dict[str, Any]:
    """调用阿里通义千问 API（OpenAI 兼容格式）"""
    api_base = self.api_base or 'https://dashscope.aliyuncs.com/compatible-mode/v1'
    url = f"{api_base}/chat/completions"
    
    payload = {
        "model": self.model_name,  # qwen-max, qwen-turbo
        "messages": messages,
        "temperature": self.temperature,
        "max_tokens": self.max_tokens,
    }
    
    # Function Calling (使用 tools 格式)
    if functions:
        payload["tools"] = [
            {"type": "function", "function": func} 
            for func in functions
        ]
        if function_call and function_call != "none":
            payload["tool_choice"] = "auto"
    
    headers = {
        "Authorization": f"Bearer {self.api_key}",
        "Content-Type": "application/json"
    }
    
    response = requests.post(url, json=payload, headers=headers, timeout=60)
    response.raise_for_status()
    
    result = response.json()
    choice = result["choices"][0]
    message = choice["message"]
    
    output = {}
    if message.get("content"):
        output["response"] = message["content"]
    
    # 处理 tool_calls
    if message.get("tool_calls"):
        tool_call = message["tool_calls"][0]
        output["function_call"] = {
            "name": tool_call["function"]["name"],
            "arguments": json.loads(tool_call["function"]["arguments"])
        }
    
    return output
```

## 文心一言集成

### 1. 配置

```python
# .env
QIANFAN_ACCESS_KEY=your-access-key
QIANFAN_SECRET_KEY=your-secret-key
```

### 2. 调用实现

```python
def _call_wenxin_api(
    self,
    messages: List[Dict[str, str]],
    functions: Optional[List[Dict]] = None,
    function_call: Optional[str] = None
) -> Dict[str, Any]:
    """调用百度文心一言 API"""
    import qianfan
    
    # 初始化千帆SDK
    chat = qianfan.ChatCompletion(
        ak=os.getenv("QIANFAN_ACCESS_KEY"),
        sk=os.getenv("QIANFAN_SECRET_KEY")
    )
    
    # 调用
    response = chat.do(
        model=self.model_name,  # ERNIE-Bot-4, ERNIE-Bot-turbo
        messages=messages,
        temperature=self.temperature,
        max_output_tokens=self.max_tokens,
    )
    
    return {
        "response": response["result"]
    }
```

## Function Calling

### 1. 什么是 Function Calling？

Function Calling 允许大模型根据用户输入，自动选择并调用预定义的函数。

```
用户输入: "北京今天天气怎么样？"
    ↓
大模型理解意图
    ↓
选择函数: get_weather(location="北京")
    ↓
执行函数，获取天气数据
    ↓
大模型生成回复: "北京今天晴天，温度20-28度..."
```

### 2. 定义函数

```python
# 定义可用的函数
functions = [
    {
        "name": "get_weather",
        "description": "获取指定城市的天气信息",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "城市名称，例如：北京、上海"
                },
                "unit": {
                    "type": "string",
                    "enum": ["celsius", "fahrenheit"],
                    "description": "温度单位"
                }
            },
            "required": ["location"]
        }
    },
    {
        "name": "search_devices",
        "description": "搜索IoT设备",
        "parameters": {
            "type": "object",
            "properties": {
                "keyword": {
                    "type": "string",
                    "description": "搜索关键词"
                },
                "device_type": {
                    "type": "string",
                    "description": "设备类型"
                }
            },
            "required": ["keyword"]
        }
    }
]
```

### 3. 调用示例

```python
# 用户消息
messages = [
    {"role": "user", "content": "帮我查一下北京今天的天气"}
]

# 调用大模型
result = llm_service.chat(messages, functions=functions)

# 检查是否需要调用函数
if "function_call" in result:
    function_name = result["function_call"]["name"]
    arguments = result["function_call"]["arguments"]
    
    # 执行对应的函数
    if function_name == "get_weather":
        weather_data = get_weather(**arguments)
        
        # 将结果返回给大模型
        messages.append({
            "role": "function",
            "name": function_name,
            "content": json.dumps(weather_data)
        })
        
        # 再次调用，生成最终回复
        final_result = llm_service.chat(messages)
        print(final_result["response"])
```

### 4. 完整示例

```python
def chat_with_functions(user_input: str):
    """支持 Function Calling 的对话"""
    messages = [
        {"role": "system", "content": "你是一个智能助手"},
        {"role": "user", "content": user_input}
    ]
    
    # 第一次调用
    result = llm_service.chat(messages, functions=functions)
    
    # 如果需要调用函数
    if "function_call" in result:
        function_name = result["function_call"]["name"]
        arguments = result["function_call"]["arguments"]
        
        # 执行函数
        if function_name == "get_weather":
            function_result = get_weather(**arguments)
        elif function_name == "search_devices":
            function_result = search_devices(**arguments)
        else:
            function_result = {"error": "未知函数"}
        
        # 添加函数结果到消息历史
        messages.append({
            "role": "assistant",
            "content": None,
            "function_call": result["function_call"]
        })
        messages.append({
            "role": "function",
            "name": function_name,
            "content": json.dumps(function_result, ensure_ascii=False)
        })
        
        # 第二次调用，生成最终回复
        final_result = llm_service.chat(messages)
        return final_result["response"]
    
    # 直接返回回复
    return result.get("response", "")
```

## 流式输出

### 1. OpenAI 流式调用

```python
def chat_stream(messages: List[Dict[str, str]]):
    """流式输出"""
    url = f"{self.api_base}/chat/completions"
    
    payload = {
        "model": self.model_name,
        "messages": messages,
        "stream": True  # 启用流式输出
    }
    
    headers = {
        "Authorization": f"Bearer {self.api_key}",
        "Content-Type": "application/json"
    }
    
    response = requests.post(
        url, 
        json=payload, 
        headers=headers, 
        stream=True
    )
    
    for line in response.iter_lines():
        if line:
            line = line.decode('utf-8')
            if line.startswith('data: '):
                data = line[6:]
                if data == '[DONE]':
                    break
                
                chunk = json.loads(data)
                delta = chunk['choices'][0]['delta']
                if 'content' in delta:
                    yield delta['content']
```

### 2. FastAPI 流式响应

```python
from fastapi.responses import StreamingResponse

@app.post("/api/chat/stream")
async def chat_stream_endpoint(request: ChatRequest):
    """流式对话接口"""
    async def generate():
        for chunk in llm_service.chat_stream(request.messages):
            yield f"data: {json.dumps({'content': chunk})}\n\n"
        yield "data: [DONE]\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

### 3. 前端接收流式响应

```javascript
async function chatStream(messages) {
  const response = await fetch('/api/chat/stream', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${token}`
    },
    body: JSON.stringify({ messages })
  })
  
  const reader = response.body.getReader()
  const decoder = new TextDecoder()
  
  while (true) {
    const { done, value } = await reader.read()
    if (done) break
    
    const chunk = decoder.decode(value)
    const lines = chunk.split('\n')
    
    for (const line of lines) {
      if (line.startsWith('data: ')) {
        const data = line.slice(6)
        if (data === '[DONE]') return
        
        const json = JSON.parse(data)
        console.log(json.content)  // 输出每个字符
      }
    }
  }
}
```

## 成本控制

### 1. Token 计数

```python
import tiktoken

def count_tokens(text: str, model: str = "gpt-3.5-turbo") -> int:
    """计算文本的 token 数量"""
    encoding = tiktoken.encoding_for_model(model)
    tokens = encoding.encode(text)
    return len(tokens)

# 计算消息的 token
def count_message_tokens(messages: List[Dict]) -> int:
    total = 0
    for message in messages:
        total += count_tokens(message["content"])
        total += 4  # 每条消息的元数据
    total += 2  # 请求的开始和结束
    return total
```

### 2. 成本估算

```python
# 模型价格（每1K tokens，单位：元）
PRICING = {
    "gpt-4": {"input": 0.21, "output": 0.42},
    "gpt-3.5-turbo": {"input": 0.01, "output": 0.02},
    "qwen-max": {"input": 0.08, "output": 0.08},
    "wenxin-4": {"input": 0.12, "output": 0.12},
}

def estimate_cost(model: str, input_tokens: int, output_tokens: int) -> float:
    """估算调用成本"""
    pricing = PRICING.get(model, {"input": 0, "output": 0})
    cost = (
        (input_tokens / 1000) * pricing["input"] +
        (output_tokens / 1000) * pricing["output"]
    )
    return round(cost, 4)
```

### 3. 使用记录

```sql
-- LLM 调用记录表
CREATE TABLE `llm_usage_logs` (
  `id` INT PRIMARY KEY AUTO_INCREMENT,
  `user_id` INT,
  `model_id` INT,
  `prompt_tokens` INT,
  `completion_tokens` INT,
  `total_tokens` INT,
  `cost` DECIMAL(10,4),
  `created_at` DATETIME DEFAULT CURRENT_TIMESTAMP,
  INDEX `idx_user_id` (`user_id`),
  INDEX `idx_created_at` (`created_at`)
);
```

```python
# 记录使用情况
def log_llm_usage(user_id, model_id, usage_data, cost):
    log = LLMUsageLog(
        user_id=user_id,
        model_id=model_id,
        prompt_tokens=usage_data.get("prompt_tokens", 0),
        completion_tokens=usage_data.get("completion_tokens", 0),
        total_tokens=usage_data.get("total_tokens", 0),
        cost=cost
    )
    db.add(log)
    db.commit()
```

## 错误处理

### 1. 常见错误

```python
def chat_with_retry(messages, max_retries=3):
    """带重试的调用"""
    for attempt in range(max_retries):
        try:
            return llm_service.chat(messages)
        
        except requests.exceptions.Timeout:
            # 超时错误
            logger.warning(f"请求超时，重试 {attempt + 1}/{max_retries}")
            time.sleep(2 ** attempt)  # 指数退避
        
        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code
            
            if status_code == 401:
                # API Key 错误
                raise ValueError("API Key 无效或已过期")
            
            elif status_code == 429:
                # 速率限制
                retry_after = int(e.response.headers.get("Retry-After", 60))
                logger.warning(f"达到速率限制，等待 {retry_after} 秒")
                time.sleep(retry_after)
            
            elif status_code == 500:
                # 服务器错误
                logger.error("大模型服务器错误，重试中...")
                time.sleep(5)
            
            else:
                raise
        
        except Exception as e:
            logger.error(f"未知错误: {e}")
            if attempt == max_retries - 1:
                raise
            time.sleep(2)
    
    raise Exception("达到最大重试次数")
```

### 2. 降级策略

```python
def chat_with_fallback(messages, primary_model, fallback_model):
    """主备模型切换"""
    try:
        # 优先使用主模型
        service = LLMService(primary_model)
        return service.chat(messages)
    
    except Exception as e:
        logger.warning(f"主模型调用失败: {e}，切换到备用模型")
        
        # 降级到备用模型
        service = LLMService(fallback_model)
        return service.chat(messages)
```

## 最佳实践

### 1. Prompt 工程

```python
# ✅ 好的 Prompt
messages = [
    {
        "role": "system",
        "content": """你是一个专业的AI助手，擅长回答技术问题。
要求：
1. 回答要准确、专业
2. 使用Markdown格式
3. 如果不确定，明确说明
4. 回答要简洁，控制在500字以内"""
    },
    {
        "role": "user",
        "content": "什么是Docker？"
    }
]

# ❌ 不好的 Prompt
messages = [
    {"role": "user", "content": "Docker"}
]
```

### 2. 上下文管理

```python
def manage_context(messages, max_tokens=4000):
    """管理对话上下文，避免超出限制"""
    total_tokens = 0
    kept_messages = []
    
    # 始终保留系统消息
    if messages[0]["role"] == "system":
        kept_messages.append(messages[0])
        total_tokens += count_tokens(messages[0]["content"])
        messages = messages[1:]
    
    # 从最新消息开始保留
    for message in reversed(messages):
        tokens = count_tokens(message["content"])
        if total_tokens + tokens > max_tokens:
            break
        kept_messages.insert(1, message)
        total_tokens += tokens
    
    return kept_messages
```

### 3. 缓存优化

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_chat(messages_tuple, model_name):
    """缓存相同的请求"""
    messages = list(messages_tuple)
    return llm_service.chat(messages)

# 使用时转换为 tuple
messages_tuple = tuple(
    (msg["role"], msg["content"]) 
    for msg in messages
)
result = cached_chat(messages_tuple, model.name)
```

## 教学要点总结

### 核心概念
1. **统一接口**: 多模型统一调用
2. **Function Calling**: 函数调用能力
3. **流式输出**: 实时响应用户
4. **Token 计数**: 成本控制
5. **错误处理**: 重试和降级

### 实用技能
- ✅ 集成多种大模型API
- ✅ 实现 Function Calling
- ✅ 流式输出处理
- ✅ Token 使用统计
- ✅ 成本估算和控制

### 最佳实践
- ✅ 优秀的 Prompt 设计
- ✅ 上下文长度管理
- ✅ 请求缓存优化
- ✅ 错误重试机制
- ✅ 主备模型切换

## 相关文档

- [后端架构](../02_系统架构/后端架构.md) - 后端服务设计
- [环境配置](../03_部署运维/环境配置.md) - API Key 配置
- [常见问题排查](../04_开发调试/常见问题排查.md) - API 调用问题
